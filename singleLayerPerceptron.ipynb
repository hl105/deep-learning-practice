{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hl105/deep-learning-practice/blob/main/singleLayerPerceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RS4H2YtZNey2",
        "outputId": "d3a259dc-7fc6-4f95-b3fb-b4f32ac8dd4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8816, 785)\n",
            "[0 1]\n",
            "(6612, 784)\n",
            "(2204, 784)\n",
            "(6612,)\n",
            "(2204,)\n",
            "Values before rescaling:  [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
            " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
            " 252 253 254 255]\n",
            "Values after rescaling:  [0.         0.00392157 0.00784314 0.01176471 0.01568627 0.01960784\n",
            " 0.02352941 0.02745098 0.03137255 0.03529412 0.03921569 0.04313725\n",
            " 0.04705882 0.05098039 0.05490196 0.05882353 0.0627451  0.06666667\n",
            " 0.07058824 0.0745098  0.07843137 0.08235294 0.08627451 0.09019608\n",
            " 0.09411765 0.09803922 0.10196078 0.10588235 0.10980392 0.11372549\n",
            " 0.11764706 0.12156863 0.1254902  0.12941176 0.13333333 0.1372549\n",
            " 0.14117647 0.14509804 0.14901961 0.15294118 0.15686275 0.16078431\n",
            " 0.16470588 0.16862745 0.17254902 0.17647059 0.18039216 0.18431373\n",
            " 0.18823529 0.19215686 0.19607843 0.2        0.20392157 0.20784314\n",
            " 0.21176471 0.21568627 0.21960784 0.22352941 0.22745098 0.23137255\n",
            " 0.23529412 0.23921569 0.24313725 0.24705882 0.25098039 0.25490196\n",
            " 0.25882353 0.2627451  0.26666667 0.27058824 0.2745098  0.27843137\n",
            " 0.28235294 0.28627451 0.29019608 0.29411765 0.29803922 0.30196078\n",
            " 0.30588235 0.30980392 0.31372549 0.31764706 0.32156863 0.3254902\n",
            " 0.32941176 0.33333333 0.3372549  0.34117647 0.34509804 0.34901961\n",
            " 0.35294118 0.35686275 0.36078431 0.36470588 0.36862745 0.37254902\n",
            " 0.37647059 0.38039216 0.38431373 0.38823529 0.39215686 0.39607843\n",
            " 0.4        0.40392157 0.40784314 0.41176471 0.41568627 0.41960784\n",
            " 0.42352941 0.42745098 0.43137255 0.43529412 0.43921569 0.44313725\n",
            " 0.44705882 0.45098039 0.45490196 0.45882353 0.4627451  0.46666667\n",
            " 0.47058824 0.4745098  0.47843137 0.48235294 0.48627451 0.49019608\n",
            " 0.49411765 0.49803922 0.50196078 0.50588235 0.50980392 0.51372549\n",
            " 0.51764706 0.52156863 0.5254902  0.52941176 0.53333333 0.5372549\n",
            " 0.54117647 0.54509804 0.54901961 0.55294118 0.55686275 0.56078431\n",
            " 0.56470588 0.56862745 0.57254902 0.57647059 0.58039216 0.58431373\n",
            " 0.58823529 0.59215686 0.59607843 0.6        0.60392157 0.60784314\n",
            " 0.61176471 0.61568627 0.61960784 0.62352941 0.62745098 0.63137255\n",
            " 0.63529412 0.63921569 0.64313725 0.64705882 0.65098039 0.65490196\n",
            " 0.65882353 0.6627451  0.66666667 0.67058824 0.6745098  0.67843137\n",
            " 0.68235294 0.68627451 0.69019608 0.69411765 0.69803922 0.70196078\n",
            " 0.70588235 0.70980392 0.71372549 0.71764706 0.72156863 0.7254902\n",
            " 0.72941176 0.73333333 0.7372549  0.74117647 0.74509804 0.74901961\n",
            " 0.75294118 0.75686275 0.76078431 0.76470588 0.76862745 0.77254902\n",
            " 0.77647059 0.78039216 0.78431373 0.78823529 0.79215686 0.79607843\n",
            " 0.8        0.80392157 0.80784314 0.81176471 0.81568627 0.81960784\n",
            " 0.82352941 0.82745098 0.83137255 0.83529412 0.83921569 0.84313725\n",
            " 0.84705882 0.85098039 0.85490196 0.85882353 0.8627451  0.86666667\n",
            " 0.87058824 0.8745098  0.87843137 0.88235294 0.88627451 0.89019608\n",
            " 0.89411765 0.89803922 0.90196078 0.90588235 0.90980392 0.91372549\n",
            " 0.91764706 0.92156863 0.9254902  0.92941176 0.93333333 0.9372549\n",
            " 0.94117647 0.94509804 0.94901961 0.95294118 0.95686275 0.96078431\n",
            " 0.96470588 0.96862745 0.97254902 0.97647059 0.98039216 0.98431373\n",
            " 0.98823529 0.99215686 0.99607843 1.        ]\n",
            "Epoch: 1, loss: 0.013914095583787053\n",
            "Epoch: 2, loss: 0.008620689655172414\n",
            "Epoch: 3, loss: 0.006352087114337568\n",
            "Epoch: 4, loss: 0.00529340592861464\n",
            "Epoch: 5, loss: 0.003932244404113733\n",
            "Epoch: 6, loss: 0.003176043557168784\n",
            "Epoch: 7, loss: 0.0030248033877797943\n",
            "Epoch: 8, loss: 0.0027223230490018148\n",
            "Epoch: 9, loss: 0.0024198427102238356\n",
            "Epoch: 10, loss: 0.002268602540834846\n",
            "Epoch: 11, loss: 0.002117362371445856\n",
            "Epoch: 12, loss: 0.001663641863278887\n",
            "Epoch: 13, loss: 0.0015124016938898972\n",
            "Epoch: 14, loss: 0.001663641863278887\n",
            "Epoch: 15, loss: 0.0018148820326678765\n",
            "Epoch: 16, loss: 0.0018148820326678765\n",
            "Epoch: 17, loss: 0.0018148820326678765\n",
            "Epoch: 18, loss: 0.0013611615245009074\n",
            "Epoch: 19, loss: 0.0013611615245009074\n",
            "Epoch: 20, loss: 0.0015124016938898972\n",
            "Epoch: 21, loss: 0.001058681185722928\n",
            "Epoch: 22, loss: 0.0012099213551119178\n",
            "Epoch: 23, loss: 0.0009074410163339383\n",
            "Epoch: 24, loss: 0.0015124016938898972\n",
            "Epoch: 25, loss: 0.0009074410163339383\n",
            "Epoch: 26, loss: 0.0012099213551119178\n",
            "Epoch: 27, loss: 0.0007562008469449486\n",
            "Accuracy:  0.9981851179673321\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAXOElEQVR4nO3caXQdBJnG8ecmabaSNGmbtE2Tplu60Y2GAi2lSAVlk6Ig68CUXUTFiozbjMdxFEUHRBBBdFBAGAQVK0tByiJg2Vq60n0jXdNm35r9zrf3HM7MObnPPYejZ87/9/n+701vlqf3y5tIJpNJAQAgKePv/QUAAP5xMAoAgMAoAAACowAACIwCACAwCgCAwCgAAAKjAAAIWak+8PZNZ9pP/uQHc+ymfuswu5Gk0cfW2s3sYfvSei3X8hXH282PL3gkrdf69vufsptpJf57t7e1yG4O7iyxG0m655MP2c13tpxnN3+e+aDd5Gdk2k1uIuVfuw/50v6FdjN98AG72dnpf5+e3znVbnq60nsfzp+2zm7yMnvsZmZ+jd1kJ/rsRpIO9Qyxm0dqTrSbNz9x+4CP4ZMCACAwCgCAwCgAAAKjAAAIjAIAIDAKAIDAKAAAAqMAAAiMAgAgMAoAgMAoAAACowAACIlkMplM5YETHv++/eSzKvyDc1eOXGk3kvTVVZ+1m/5+fxMHD+60m4xESm/xh7w8xz/OJkmPtU62m5fqpthNSW6b3fyi/E27kaTxf7zBbkrH19tNUe5Ru7m18nm7ea5plt1I0tPbp9tNSZH/fZo+9KDd/KjsZbs5ceX1diNJXc25dpPZ7B8uXHvJXXYz86mb7UaSskd02M2WBf7RzIyR2wd+jP2sAID/txgFAEBgFAAAgVEAAARGAQAQGAUAQGAUAACBUQAABEYBABAYBQBAYBQAAIFRAACErFQf2FefYz/52mSF3bR2L7IbSSotbrWbf534rN188Q9X283Uk3bbTUYiYTeS9PShmXZzx/jf282RvsF2M/nXN9qNJGlIv53UNRbYzZLj/IN916+80m6umpXeYcDKkka7qakrtpvX2ybYzWdaSuzm1hkv2o0k/eDZ8+3msQvusZs5Dy+1m5wq/wChJF0yabXdVK++yG7WnDPwY/ikAAAIjAIAIDAKAIDAKAAAAqMAAAiMAgAgMAoAgMAoAAACowAACIwCACAwCgCAwCgAAAKjAAAIiWQymUzlgdet8q9BnlDgXwf98YYz7EaSCvK77Kaupsh/oTSOl1aMP2I3+2r965aSpJS+mx+28eP3201bssduvrI3hRON/4c1B0fbTWmhf61yz/YRdrP7/AfsZtJr/u+SJPXW59lNUUVTWq/lunLC23Zz7ZAtab1WR7LPbjZ1+1dzl7f4F4efWDXXbiSpeor/t/LCUv+y6mVV7wz4GD4pAAACowAACIwCACAwCgCAwCgAAAKjAAAIjAIAIDAKAIDAKAAAAqMAAAiMAgAgMAoAgJDyQbzvbvyU/eR7O/2jbl8f8aLdSNIPa/1Dev8xaoXdpLOiZ69fYjcnlNak8UrSvo4iu+lN+v+qAy2FdtPS5h90k6TrZvzNbu5/51S7KRvdYDeNbfl28+Cch+xGkq5avcRuumr9r29B9Wa72dvm/67vqSmxG0nKPjjIbhJ9/iXL3olH7eaK6f5hQEla01RhNxv2ltnN7su+OeBj+KQAAAiMAgAgMAoAgMAoAAACowAACIwCACAwCgCAwCgAAAKjAAAIjAIAIDAKAIDAKAAAQsoH8SY8/n37yc+fss5ulm2bYTeSVDa0xW4GZfbZzQfvltvNkrNftpuO/my7kaSnfneK3Sy5/AW7ebdprN1sWDHZbiQpp95vJl60zW7qfjjObg6elGU3Qzel9Cv3v9TP9I+6lbzXbze5Nxy0m107RtpNcVmz3UjSyIJWu/lc+at2c/vOM+3mnyvfshtJuqxgl90s3nKR3byy6I4BH8MnBQBAYBQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAARGAQAQGAUAQGAUAACBUQAABEYBABBSPoh39OBY+8mXtQ+3m7k5++1Gkr6x7zy7mXxMrd28cWSC3fT0ZdpNe/cgu5GkjlX+e15wwhG/ubPQbupm5tiNJPUc4zedEzvtZsg7uXaT2+AfnOvN8w/bSVJ7md/lNPivU7rKPzi37Ub/gOOwlekdfayf25tW5yoZ3WQ3l419N63Xum/jQrt5/qSf282EioGPHfJJAQAQGAUAQGAUAACBUQAABEYBABAYBQBAYBQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAISsVB845emb7CdP5PmHq5LtKX9JH/KzTzxsN6fn+Ye/pqw82W7yS9vtpqszvWNhhbUp3Tf8kPY3Suymc7adqHVKjx9Jmnpnk91svrnYbvrSuNeX8N9utVamdxBv7DNpHKpbkm83o544YDdTf+i/33u+7x8glKQR+f6xw/ZXSu3mgTN+azff3Xuu3UhSamdJP6w8Ky+t1xoInxQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAARGAQAQGAUAQGAUAACBUQAABEYBABAYBQBAYBQAACHlk6TXzH/NfvIM+af/fvPsIruRpBeaZtjNnu5DdpM1pNtu+jcMsZvcmc12I0mJfv/yZE+h/30q2JjGNdbRmXYjSe0T/Quc45/ss5vWCv/rG7LZ/z715BXZjSRlHqi3m/y9BXazc+kku8lq9y+/JlbbiSQpb5F/LbapwP95veThpXaz7tqf2o0knd19gd30JP2f8VQOAfNJAQAQGAUAQGAUAACBUQAABEYBABAYBQBAYBQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAIREMplM6VJU5YO3209eVNJmN+dUvm83kvTqoSq7+enkx+3mwuVfsJslJ79hN3++91S7kaTmUzvtZtz9/us0Tkrj8N4x/tE0SSra1Ws3tcenfOsxZPpvnTL9+4jKbvGPs0lSw4Iuu5n67w1pvZarb/9Bu6n5anVar1W5zP83Je/2j+jNKt5vN5nqtxtJGj7I/1u5uX2U3fzX3N8M+Bg+KQAAAqMAAAiMAgAgMAoAgMAoAAACowAACIwCACAwCgCAwCgAAAKjAAAIjAIAIDAKAICQ8tWwZaf/zH7yXT3D7Wbp65fYjSRVV+2xm0vfvtZujtnjH1qrmTPUbhqnp3dYK+OAf6iuY6T/WvXH99nNpF8ftRtJOnxCgd10j/Ov2+Xs8N+7gr3+e1d09V67kST/t0na/aNiuym7N9tucvr8n4eylWlcIJRUN9f/N9VvKLKb/fsr7aZtqn+0UJI2nHGv3Rz3zCf9F5o78EP4pAAACIwCACAwCgCAwCgAAAKjAAAIjAIAIDAKAIDAKAAAAqMAAAiMAgAgMAoAgMAoAABCytfdbtn5WfvJb6183m5mTUjvWNih9kK76a3Nt5vLL3/Vbpbdd6rdTFrdZjeSlNHmHxmrWVxiNyVvJe2mrdJ/vyUp/7B/dK7wSf9w4Yxvr7abitwGu1l+8Fi7kaS6l8rsJjONu4ptZX7UPK7cbhqP9X+GJGnSfYfspi93ZBqNnejt0+/2I0lvdPpHM6fO253Waw2ETwoAgMAoAAACowAACIwCACAwCgCAwCgAAAKjAAAIjAIAIDAKAIDAKAAAAqMAAAiMAgAgMAoAgJBIJpMpnSocf8ed9pMPGutf+hxa0G43knTz+Jfs5qTc/Xaz6Mmv2k3lcz12U3NGtt1IUob/UsppSthNx5yjdjPxrl67kaSuoTl2UzvXf//6cvyrnd3D+uxG2WmcLpWU0eJffi2q8q+4jrjF/zdtv6bUbvqz07uSWvGi//W1VPrvXV+u/3vRVu3/XkjSwok77Oavq6fZzZ4bB/77xScFAEBgFAAAgVEAAARGAQAQGAUAQGAUAACBUQAABEYBABAYBQBAYBQAAIFRAAAERgEAEFK+EjV0Wp395E2teXZz2sjtdiNJ3914rt30vF9oN33D/WNcrRX+cbbeoekdj5t6V7PdJJr9w4U7iivtpuN7h+xGkho7/Pc8J8P/N3X1+EfTVsy9325y/DtrkqR76hfYzat3zrOb+hPtRIW7/Gb4L97yI0nb7/a/wDHP+79PBxb4Pw+Z+3LtRpL+2jvJbsomHEnrtQbCJwUAQGAUAACBUQAABEYBABAYBQBAYBQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAARGAQAQUr74dGR/kf3kGR2ZdjNuVnpHnnp6/Ne6+vwVdvPU3ll2U99cYjeTf9lpN5K078zhdtM2tthuJj3UYje7MkfajSRNmb/bbjZsq7CbC6tX2c3DTf5xtuKsdruRpGeemG83ncf12834P/o/e4fm5dtNRkGB3UhSMjtpN63l/nG7wfv8y4XNU/zjjZJ0z4JH7eYLr1+e1msNhE8KAIDAKAAAAqMAAAiMAgAgMAoAgMAoAAACowAACIwCACAwCgCAwCgAAAKjAAAIjAIAIKR8JWpq1X77yc8bsc5uFuXvsBtJmn1Cjd1c+rub7aY3zz/GpQL/KFlnSa7/OpJG37/Wbg5cP9tutt6YZzcZLWm8d5K2H/YPCh5btc9u/rB+jt38fMFv7ebzy5fYjSSV7PPfv85pXXYzqKHDbroL/Z+HxvOOtRtJKtzi/1926OajdtP6jVa7yWoebDeS9GZbld0kMtP7fRoInxQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAARGAQAQGAUAQGAUAACBUQAABEYBABAYBQBASPkg3uadZR/l1xFGljen1f3Lk1fYzX0XP2A31762xG7GPWonytta60eSdn9ltt0cLe+1m9yabLtZeO4au5GkFzdNs5vZaRzE+8bCZ+3mixsvtZvisY12I0l964fZTbLZ/z5tuSHHbrLa7USH56V50K2w007yPuEft+voHmQ3Gxb+ym4kacbr16TVfRT4pAAACIwCACAwCgCAwCgAAAKjAAAIjAIAIDAKAIDAKAAAAqMAAAiMAgAgMAoAgMAoAAACowAACClfSVVfwn7yRyf+3m7+2DbebiTp84uX2821L11tN1UP9dhNT4F/bXH/+WPsRpKyOvxm6j3+ZdraBUPtZsXKWXYjSXm1/v9dntp6it30X+T/jLe05ttNYn+u3UhS6QX+5dwx9/iXVWvn+j+vxZv77ebgGX12I0mJRv/y68SJdXbz5jtT7GbVzEy7kaTl835uN2c8c0tarzUQPikAAAKjAAAIjAIAIDAKAIDAKAAAAqMAAAiMAgAgMAoAgMAoAAACowAACIwCACAwCgCAkPJBvL+e9RP7yZe1+8ftWvvTOxZWkNFpN/m7/cNfe7/sv87g5/PsZtSD6+xGklRVaSc15/pH03r9O3DKKW/1I0mqLbSTwoX+8biWXv/7dNrEbXbzaqLKbiSp6Lpuu6lf6B9o6yzttZvsd5N2U7zK//2TpI5FbXbz3oFyuymdfMRuHq2fbzeS9INRr6TVfRT4pAAACIwCACAwCgCAwCgAAAKjAAAIjAIAIDAKAIDAKAAAAqMAAAiMAgAgMAoAgMAoAABCygfxtnQX209+ev4uuzn1tS/ajSRlDeqzm94R/XaT0eMfGCt9zT/OpsrRfiOpa5h/1C2jx3+dsr/5x9n2ZxT4LyQpI427af3/XWo3WTd/YDcbvz3TbnKu67AbSdr++Qq7mfjbBrtpHeMfSGy4rtF/nbrBdiNJHxuzx27eWTbDbqo/s91uWnpz7EaS7qqvtptzT3wvrdcaCJ8UAACBUQAABEYBABAYBQBAYBQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAARGAQAQGAUAQEgkk8lkKg+sfPB2+8k/Nn2r3YzKabYbSVrbVG433xm7zG4ufuEmu8k+4h/Ry25O2I0klb7XZTe7F/sX5yZ/c4Pd7PyWfzxOkhL+rUP1TzhqN6VP+cfM2sr9721vrp1IkjLSeB+yTvYP4rVv9o9f5kz2f2+L8/3vkSRdXLHKbu548Ry7GdTq/w7++OKH7EaSTs9rspt++Qc9C8v2DvgYPikAAAKjAAAIjAIAIDAKAIDAKAAAAqMAAAiMAgAgMAoAgMAoAAACowAACIwCACAwCgCAkJXqAy+tfsd+8m+W+E31Q0vtRpLOO+stu3msYZ7dTJ60325GzW6xm4M3jbEbSfrgnCF2U7Gi124SZSPsJunfjpMkFW3zm7y3s+2mbob/BY552j841zi7yG4kKcP/Nil5Wo/ddLf4h+AK8zrtZt/WUruRpDuPnG43ReMa7aarJ+U/j+GT+ekd9Fy89dN2s2T0Sru5LIXH8EkBABAYBQBAYBQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAARGAQAQGAUAQGAUAACBUQAABEYBABBSPgO4usG/2jlzpX+FdPuSe+1GkqpeutZubjvxKbt57sW5drO11L9Umfv1LruRpGEFh+ymb1OJ3ey80r+S2js0jTOfkgpq+u2mrdy/ktpR1W036Vw8rT3Z//dIkvzjpcpa718i7R3r/7xeOGqr3Zw88Wm7kaSGvmPs5veHq+1m9cbxdjP9kS/ZjSQlK/wrs4dG+BeRU8EnBQBAYBQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAARGAQAQGAUAQGAUAACBUQAABEYBABASyWQymcoDq5/7lv3kg7P9A2M1W/1Da5I0Z/ZOu9la5x8LmzXigN2seXqa3fTn2IkkKWNGs90k3/MPa007c5vd1P1gnN1I0oGFKd9tDGOXddjN9uv91xn9rN8cOCWNy3aSLjv1b3bz2Ovz7eaGj71sN+tayu2mqTvPbiRpx5uVdtMzzD/GmF3nf29HHO8fpJSk7r5Muxme3243zy28e8DH8EkBABAYBQBAYBQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAARGAQAQGAUAQGAUAACBUQAAhJQvPl0z3j/GNSjRZze3HT7LbiTp9jF/spurOq6wm96kv6PLb/yR3Zz17g12I0ldXYPsJnNmm93808i37Oa2m4rtRpJye/zDZK2z/GOMicP+YcBDn/ZfZ/643XYjSWNy6v2o3z++d1HhGruZN3i73QzLOGo3knRL8kK7+eyo1XZz2wvn201Pf3r/z546tNZuNjekdzx0IHxSAAAERgEAEBgFAEBgFAAAgVEAAARGAQAQGAUAQGAUAACBUQAABEYBABAYBQBAYBQAACHlS2Md/Tn2kz+w6WS7ydqTazeStPaEMrvZf6TIbn439RG7mb98qd1kDO61G0lKNmfbzbTpH9jNqvZxdtPUkm83knT2pPftZlR2s93Mqqqxm9t3n2k3aw+NthtJuqviWbsZdtZjdrPoBf/n9XPzXrWbLw/dZDeSdFrJNrv5z41n2M1PznnYbpa+fbHdSNL3Jv3Jbh5K+n9fU8EnBQBAYBQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAARGAQAQGAUAQGAUAACBUQAABEYBABAYBQBASCSTyWQqD5z/l6/ZT3792Nfs5qfbF9mNJDVvG2o3P1v8a7t5pnG23ZRkt9rN49uq7UaSFlbusJu/vDfDbrKLO+0mK6vPbiTpvuMetZuKzDa7uXD91XYzrqjBbva1FtmNJBXkdNnNrg3+RdZEqf+97WsbZDenzNhqN5L0+vrJdnPc1D12s/GNiXaTzLQTSdJVZ71sN7989xS7+eCqgf+O80kBABAYBQBAYBQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAARGAQAQGAUAQGAUAACBUQAAhKxUH7i4fJ395Ns6R9lN27phdiNJ/YX9dvNvWxbbzR9mPmg3b3X6R8l+Vb3RbiTp+jVX2M3XFj5rN2WDGu1mcIZ/0E2Sjs/utpuHW/yjaa0b/Z+9jpP8w3tdPSn/2n3IHVOesJsVw6fbza3D1trN2Zsuspv7K1bYjSRN31plNzsahtvNqR9fbzev7JhkN5L0qzUn2032If8IYSr4pAAACIwCACAwCgCAwCgAAAKjAAAIjAIAIDAKAIDAKAAAAqMAAAiMAgAgMAoAgMAoAABCIplMJv/eXwQA4B8DnxQAAIFRAAAERgEAEBgFAEBgFAAAgVEAAARGAQAQGAUAQGAUAADhfwAuUq8kVs2maAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# binary classifications (0 and 1) using a single layer perceptron.\n",
        "# extrack data with digit 0 and 1\n",
        "\n",
        "\n",
        "def forward_propagation(x,y,weights,bias):\n",
        "  # x: training data as verctory nparray\n",
        "  # y: binary label\n",
        "\n",
        "  y_pred = predict(x,weights,bias)\n",
        "  loss = (y_pred - y)**2 #we want to minimize this\n",
        "  d_loss = 2*(y_pred - y) #derivative (useful for backpropagation)\n",
        "\n",
        "  return y_pred, loss, d_loss\n",
        "\n",
        "# Backpropagation: gradient based learning\n",
        "# multiply the gradient of each layer to obtain the whole gradient: chain rule\n",
        "def backpropagation(x, d_loss):\n",
        "  gradients = list()\n",
        "  for feature_value in x:\n",
        "    gradients.append(d_loss*feature_value)\n",
        "  return gradients\n",
        "\n",
        "# Optimization: optimizes the perceptron's weights every epoch\n",
        "def optimization(x,y, learning_rate):\n",
        "  epoch = 0\n",
        "  error = 999\n",
        "\n",
        "  #initialize wieght and bias randomly\n",
        "  weights = np.random.rand(x.shape[1])\n",
        "  bias = np.random.rand()\n",
        "\n",
        "  #keep track\n",
        "  errors = list()\n",
        "  epochs = list()\n",
        "\n",
        "  while(epoch <=30) and (error > 9e-4):\n",
        "    total_loss = 0\n",
        "    for i in range(x.shape[0]):\n",
        "      #forward propagation\n",
        "      y_pred, loss, d_loss = forward_propagation(x[i],y[i],weights, bias)\n",
        "      #backpropagation\n",
        "      gradients = backpropagation(x[i], d_loss)\n",
        "      #update weights\n",
        "      weights = weights - (learning_rate*np.array(gradients))\n",
        "\n",
        "    #evaluation\n",
        "    for index, feature_value_test in enumerate(x):\n",
        "      y_pred, loss, d_loss = forward_propagation(feature_value_test, y[index], weights, bias)\n",
        "      total_loss += loss\n",
        "\n",
        "    errors.append(total_loss/len(x)) #MSE\n",
        "    epochs.append(epoch)\n",
        "    error = errors[-1]\n",
        "    epoch+=1\n",
        "    print(f\"Epoch: {epoch}, loss: {errors[-1]}\")\n",
        "\n",
        "  return weights, bias, errors\n",
        "\n",
        "def activation_function(prediction):\n",
        "  #gets output(prediction) of perceptron's function & applies activation function\n",
        "  #maps negative outputs to 0 & positive outputs to 1\n",
        "  if prediction >= 0:\n",
        "    return 1\n",
        "  return 0\n",
        "\n",
        "def predict(x, weights, bias):\n",
        "  #predicts label of data point x by applying perceptron's & activation f\n",
        "  prediction = np.dot(weights,x) + bias\n",
        "  return activation_function(prediction)\n",
        "\n",
        "\n",
        "def calculate_accuracy(x_test, y_test, weights, bias):\n",
        "\n",
        "    # Initialize True Positive, True Negative, False Positive and False Negative\n",
        "    tp, tn, fp, fn = 0, 0, 0, 0\n",
        "\n",
        "    for sample, label in zip(x_test, y_test):\n",
        "\n",
        "        prediction = predict(sample, weights, bias)\n",
        "\n",
        "        if prediction == label:\n",
        "            if prediction == 1:\n",
        "                tp += 1\n",
        "            else:\n",
        "                tn += 1\n",
        "        else:\n",
        "            if prediction == 1:\n",
        "                fp += 1\n",
        "            else:\n",
        "                fn += 1\n",
        "\n",
        "    accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
        "    return accuracy\n",
        "\n",
        "data = pd.read_csv('data_mnist.zip')\n",
        "\n",
        "# Take only data with labels 1\n",
        "data_ones = data[data['label'] == 1]\n",
        "\n",
        "# Take only data with labels 0\n",
        "data_zeros = data[data['label'] == 0]\n",
        "\n",
        "# Concatenate instances with label 0 and 1\n",
        "data = pd.concat([data_ones, data_zeros])\n",
        "print(data.shape)\n",
        "print(np.unique(data['label'].to_numpy()))\n",
        "\n",
        "\n",
        "# Split dataset with 75% training data and 25% test data\n",
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=1, shuffle=True)\n",
        "\n",
        "# Split datasets into features and labels\n",
        "x_train = train_data.drop('label', axis=1).to_numpy()\n",
        "x_test = test_data.drop('label', axis=1).to_numpy()\n",
        "y_train = train_data['label'].to_numpy()\n",
        "y_test = test_data['label'].to_numpy()\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print('Values before rescaling: ', np.unique(x_train))\n",
        "\n",
        "# Rescale data points to values between 0 and 1 (pixels are originally 0-255)\n",
        "x_train = x_train / 255.\n",
        "x_test = x_test / 255.\n",
        "print('Values after rescaling: ', np.unique(x_train))\n",
        "\n",
        "weights, bias, errors = optimization(x_train, y_train, learning_rate=0.001)\n",
        "\n",
        "acc = calculate_accuracy(x_test, y_test, weights, bias)\n",
        "print('Accuracy: ', acc)\n",
        "\n",
        "weights.resize((28, 28))\n",
        "plt.imshow(weights)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNutHTdXhN42uzkRq7zdfzd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}